{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Projet sous Spark : accidents de la circulation en France entre 2012 et 2018\n",
    "\n",
    "\n",
    "Auteur : Ivanhoé Botcazou\n",
    "\n",
    "\n",
    "Date : 6 novembre 2023\n",
    "\n",
    "\n",
    "## Prédiction des données avec les outils de Spark\n",
    "\n",
    "Dans cette seconde partie nous aimerions expliquer à l'aide des données étudiées précédemment les risques de mort, blessure graves et autres pour les utilitaires de la route en France Métropolitaine. \n",
    "\n",
    "Grandes étapes de notre travail :\n",
    "\n",
    "* Sélection des données et choix des variables : concaténation d'un tableau source.\n",
    "* Gestion des valeurs manquantes. \n",
    "* Choix d'un modèle et entraînement. \n",
    "* Test sur un échantillon final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "\n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn\n",
    "import copy \n",
    "\n",
    "from pyspark.sql.types import StringType #Type pour une colonne \n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/17 19:14:32 WARN Utils: Your hostname, ibotcazou-Latitude-7480 resolves to a loopback address: 127.0.1.1; using 192.168.1.15 instead (on interface wlp2s0)\n",
      "23/11/17 19:14:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/17 19:14:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Chargement des données\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName('Botcazou').getOrCreate() #initialiser l'environement Spark\n",
    "\n",
    "\n",
    "path = '/home/ibotcazou/Bureau/Master_data_science/DATAS_M2/Informatique_charbonel_Marie/DATA_Marie/projet_spark'\n",
    "\n",
    "Annees = range(2012,2019)\n",
    "\n",
    "car,usa,lieux,vehi = {},{},{},{} #Dico qui vont contenir les DataFrames Spark\n",
    "\n",
    "for a in Annees:\n",
    "    car[f'car_{a}'] = spark.read.load(path + f\"/caracteristiques_{a}.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    usa[f'usa_{a}'] = spark.read.load(path + f\"/usagers_{a}.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    lieux[f'lieux_{a}'] = spark.read.load(path + f\"/lieux_{a}.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    vehi[f'vehi_{a}'] = spark.read.load(path + f\"/vehicules_{a}.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat data in a large data set\n",
    "\n",
    "car_12_18 = car['car_2012'] \n",
    "usa_12_18 = usa['usa_2012'].withColumn(\"âge\", 2012 - col(\"an_nais\")) # add a column age \n",
    "lieux_12_18 = lieux['lieux_2012']\n",
    "vehi_12_18 = vehi['vehi_2012']  \n",
    "\n",
    "for a in Annees[1:]:  # Commencer à partir du deuxième élément, car le premier est déjà dans df_12_18\n",
    "    car_12_18 = car_12_18.unionByName(car[f'car_{a}'])\n",
    "    usa_12_18 = usa_12_18.unionByName(usa[f'usa_{a}'].withColumn(\"âge\", a - col(\"an_nais\")))\n",
    "    lieux_12_18 = lieux_12_18.unionByName(lieux[f'lieux_{a}'] )\n",
    "    vehi_12_18 = vehi_12_18.unionByName(vehi[f'vehi_{a}'])\n",
    "\n",
    "car_12_18 = car_12_18.withColumnRenamed(\"col\",\"coli\") #change name of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "| an|mois|jour|hrmn|lum|agg|int|atm|coli|com|catv|obs|obsm|choc|catu|sexe|trajet|âge|catr|circ|nbv|vosp|prof|plan|lartpc|larrout|surf|infra|situ|env1|grav|            features|\n",
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "| 12|   3|  16|1930|  5|  2|  1|  1|   6| 11|   7|  0|   1|   7|   1|   2|     5| 73|   3|   2|  0|   0|   1|   1|     0|     72|   1|    0|   1|   0|   1|[12.0,3.0,16.0,19...|\n",
      "| 12|   3|  16|1930|  5|  2|  1|  1|   6| 11|   7|  0|   1|   7|   3|   1|     5|  4|   3|   2|  0|   0|   1|   1|     0|     72|   1|    0|   1|   0|   4|[12.0,3.0,16.0,19...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|   7|  0|   2|   3|   1|   1|     5| 18|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|   7|  0|   2|   3|   2|   1|     0| 20|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|   7|  0|   2|   3|   1|   1|     5| 36|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|   7|  0|   2|   3|   2|   2|     0| 40|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   3|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|  33|  0|   2|   8|   1|   1|     5| 18|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|  33|  0|   2|   8|   2|   1|     0| 20|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|  33|  0|   2|   8|   1|   1|     5| 36|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   1|[12.0,9.0,1.0,214...|\n",
      "| 12|   9|   1|2145|  5|  2|  1|  1|   3| 11|  33|  0|   2|   8|   2|   2|     0| 40|   3|   2|  2|   0|   1|   1|     0|     65|   1|    0|   1|  99|   3|[12.0,9.0,1.0,214...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   1|   1|   2|     5| 50|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   1|   1|   2|     5| 57|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   1|   2|   1|     0| 56|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   1|   1|   1|     5| 22|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   1|   2|   1|     0| 30|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   3|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   8|   1|   2|     5| 50|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   8|   1|   2|     5| 57|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   8|   2|   1|     0| 56|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   8|   1|   1|     5| 22|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   1|[12.0,11.0,20.0,1...|\n",
      "| 12|  11|  20|1815|  5|  2|  1|  1|   5|670|   7|  0|   2|   8|   2|   1|     0| 30|   3|   2|  2|   0|   1|   3|     0|     77|   1|    0|   1|   0|   3|[12.0,11.0,20.0,1...|\n",
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 114:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "joindata = car_12_18.alias('c').join(vehi_12_18.alias('v'),col(\"c.Num_Acc\") == col(\"v.Num_Acc\")).join(usa_12_18.alias('u'),col(\"c.Num_Acc\") == col(\"u.Num_Acc\")).join(lieux_12_18.alias('l'),col(\"c.Num_Acc\") == col(\"l.Num_Acc\"))\n",
    "\n",
    "features_cols = [\"an\",\"mois\",\"jour\",\"hrmn\",\"lum\",\"agg\",\"int\", \"atm\",\"coli\",\"com\",\n",
    "    \"catv\",\"obs\",\"obsm\",\"choc\",\"catu\",\"sexe\",\"trajet\",\"âge\",\"catr\",\n",
    "    \"circ\",\"nbv\",\"vosp\",\"prof\",\"plan\",\"lartpc\",\"larrout\",\"surf\",\"infra\",\"situ\",\"env1\"]\n",
    "    \n",
    "target_col = \"grav\"\n",
    "\n",
    "data = joindata.select(features_cols + [\"grav\"]).filter(col('gps')=='M')\n",
    "\n",
    "#Permet de gérer le valeur manquantes en mettant la moyenne à la place ou encore la médiane \n",
    "imputer = Imputer(inputCols=features_cols, outputCols=features_cols)\n",
    "data = imputer.fit(data).transform(data)\n",
    "\n",
    "# Créez un assembleur de vecteurs\n",
    "vector_assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "\n",
    "# Transformez les données en utilisant l'assembleur de vecteurs\n",
    "data = vector_assembler.transform(data)\n",
    "\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "| an|mois|jour|hrmn|lum|agg|int|atm|coli|com|catv|obs|obsm|choc|catu|sexe|trajet|âge|catr|circ|nbv|vosp|prof|plan|lartpc|larrout|surf|infra|situ|env1|grav|            features|\n",
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "| 12|   1|   1|  15|  5|  2|  1|  2|   7|487|   7|  2|   0|   8|   1|   1|     5| 20|   4|   2|  2|   0|   1|   1|     0|     60|   2|    0|   4|  99|   3|[12.0,1.0,1.0,15....|\n",
      "| 12|   1|   1|  15|  5|  2|  1|  2|   7|487|   7|  2|   0|   8|   2|   1|     0| 20|   4|   2|  2|   0|   1|   1|     0|     60|   2|    0|   4|  99|   1|[12.0,1.0,1.0,15....|\n",
      "| 12|   1|   1|  45|  3|  1|  1|  1|   2|166|   7|  0|   2|   5|   1|   1|     4| 41|   1|   3|  3|   3|   1|   1|    20|    134|   9|    0|   1|  99|   3|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  3|  1|  1|  1|   2|166|   7|  0|   2|   5|   1|   1|     5| 57|   1|   3|  3|   3|   1|   1|    20|    134|   9|    0|   1|  99|   3|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  3|  1|  1|  1|   2|166|   7|  0|   2|   5|   2|   1|     0| 57|   1|   3|  3|   3|   1|   1|    20|    134|   9|    0|   1|  99|   2|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  3|  1|  1|  1|   2|166|  10|  0|   2|   3|   1|   1|     5| 57|   1|   3|  3|   3|   1|   1|    20|    134|   9|    0|   1|  99|   3|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   1|   1|   1|     9| 29|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   1|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   1|   1|   1|     9| 53|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   1|   2|   1|     0| 79|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   1|   2|   1|     0| 94|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   4|   1|   1|     9| 53|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   4|   2|   1|     0| 79|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1|  45|  5|  2|  1|  1|   2| 55|   7|  0|   2|   4|   2|   1|     0| 94|   4|   1|  2|   0|   1|   2|     0|     74|   1|    1|   1|   0|   4|[12.0,1.0,1.0,45....|\n",
      "| 12|   1|   1| 125|  5|  2|  1|  2|   6|557|   7|  2|   0|   9|   1|   1|     5| 34|   3|   2|  2|   0|   1|   1|     0|     85|   2|    0|   1|  99|   4|[12.0,1.0,1.0,125...|\n",
      "| 12|   1|   1| 125|  5|  2|  1|  2|   6|557|   7|  2|   0|   9|   2|   2|     5| 32|   3|   2|  2|   0|   1|   1|     0|     85|   2|    0|   1|  99|   4|[12.0,1.0,1.0,125...|\n",
      "| 12|   1|   1| 215|  5|  2|  1|  3|   1| 18|   7|  0|   2|   1|   1|   2|     0| 29|   3|   2|  2|   0|   1|   1|    31|     61|   2|    0|   1|   0|   1|[12.0,1.0,1.0,215...|\n",
      "| 12|   1|   1| 215|  5|  2|  1|  3|   1| 18|   7|  0|   2|   1|   2|   1|     0| 21|   3|   2|  2|   0|   1|   1|    31|     61|   2|    0|   1|   0|   1|[12.0,1.0,1.0,215...|\n",
      "| 12|   1|   1| 215|  5|  2|  1|  3|   1| 18|   7|  0|   2|   1|   2|   1|     0| 28|   3|   2|  2|   0|   1|   1|    31|     61|   2|    0|   1|   0|   1|[12.0,1.0,1.0,215...|\n",
      "| 12|   1|   1| 215|  5|  2|  1|  3|   1| 18|  14|  0|   2|   3|   1|   2|     0| 29|   3|   2|  2|   0|   1|   1|    31|     61|   2|    0|   1|   0|   1|[12.0,1.0,1.0,215...|\n",
      "| 12|   1|   1| 215|  5|  2|  1|  3|   1| 18|  14|  0|   2|   3|   2|   1|     0| 21|   3|   2|  2|   0|   1|   1|    31|     61|   2|    0|   1|   0|   1|[12.0,1.0,1.0,215...|\n",
      "+---+----+----+----+---+---+---+---+----+---+----+---+----+----+----+----+------+---+----+----+---+----+----+----+------+-------+----+-----+----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fraction de données à utiliser pour l'ensemble d'entraînement \n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement et de test\n",
    "train_data, test_data = data.randomSplit([train_ratio, test_ratio], seed=42)\n",
    "\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/17 19:26:41 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 105.9 MiB so far)\n",
      "23/11/17 19:26:41 WARN BlockManager: Persisting block rdd_1273_0 to disk instead.\n",
      "23/11/17 19:26:57 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:27:15 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:27:39 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:28:11 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:28:55 WARN DAGScheduler: Broadcasting large task binary with size 1431.8 KiB\n",
      "23/11/17 19:28:56 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:29:54 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "23/11/17 19:29:56 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:31:07 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "23/11/17 19:31:08 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:32:45 WARN DAGScheduler: Broadcasting large task binary with size 1432.2 KiB\n",
      "23/11/17 19:32:46 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "23/11/17 19:32:48 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:34:44 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/17 19:34:47 WARN DAGScheduler: Broadcasting large task binary with size 17.7 MiB\n",
      "23/11/17 19:34:48 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:37:20 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "23/11/17 19:37:26 WARN DAGScheduler: Broadcasting large task binary with size 33.7 MiB\n",
      "23/11/17 19:37:28 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "23/11/17 19:40:22 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "23/11/17 19:40:29 WARN MemoryStore: Not enough space to cache rdd_1273_0 in memory! (computed 362.6 MiB so far)\n",
      "[Stage 232:========================================>                (5 + 1) / 7]\r"
     ]
    }
   ],
   "source": [
    "# Créez le modèle de forêt aléatoire\n",
    "rf_classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=target_col, numTrees=200, maxDepth=10, seed=4)\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "model = rf_classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/17 19:44:05 WARN DAGScheduler: Broadcasting large task binary with size 26.6 MiB\n",
      "[Stage 261:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.5403528376177399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Faites des prédictions sur l'ensemble de test\n",
    "predictions_train = model.transform(train_data)\n",
    "\n",
    "# Évaluez les performances du modèle\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_train)\n",
    "print(f\"Accuracy train: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/17 19:41:28 WARN DAGScheduler: Broadcasting large task binary with size 26.6 MiB\n",
      "[Stage 241:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5371797093241936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Faites des prédictions sur l'ensemble de test\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Évaluez les performances du modèle\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
