{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un dataset et fonction de coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Créez un ensemble de données avec 1000 échantillons, 20 caractéristiques et 2 classes pour la variable cible\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=K, random_state=0)\n",
    "# n_informative : nombre de caractéristiques informatives, celles qui sont réellement utiles pour prédire la classe cible.\n",
    "# n_redundant : nombre de caractéristiques redondantes générées comme des combinaisons linéaires des caractéristiques informatives.\n",
    "\n",
    "y[y==0] = -1 #permet de mettre -1 à la place de 0\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$loss\\_reg\\_log(z, y)= \\dfrac{1}{n}\\sum_{i=1}^n \\log(1 + \\exp(-y_i \\times z_i))$$\n",
    "\n",
    "$$\\nabla loss\\_reg\\_log(z, y)_j= \\dfrac{1}{n}\\times  \\dfrac{-y_j \\times \\exp(-y_j \\times z_j)}{1 + \\exp(-y_j \\times z_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ici z correspond à F(X) un vecteur de \\R^n\n",
    "def loss_reg_log(z, y):\n",
    "    return np.mean(np.log(1 + np.exp(-y * z)))\n",
    "\n",
    "def grad_loss_reg_log(z, y):\n",
    "    return (-y * np.exp(-y * z) / (1 + np.exp(-y * z))) * 1/len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation du modèle. \n",
    "\n",
    "On commence par un modèle simple comme un arbre de décision peu profond. Ici un DecisionTreeRegressor est utilisé car dans le Gradient Boosting, même pour la classification, les modèles sont souvent entraînés pour prédire des modifications de probabilité plutôt que des classes directes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle de base - un arbre de décision simple - \n",
    "#a stump : Une souche de décision est un modèle d'apprentissage automatique composé d'un arbre de décision à un niveau.\n",
    "base_model = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "# Entraînement du modèle de base sur l'ensemble d'entraînement\n",
    "base_model.fit(X_train, Y_train)\n",
    "\n",
    "# Utilisation du modèle de base pour faire des prédictions initiales\n",
    "predictions_init_train = base_model.predict(X_train)\n",
    "predictions_init_test = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train =  0.79125\n",
      "accuracy test =  0.66\n"
     ]
    }
   ],
   "source": [
    "print('accuracy train = ' , accuracy_score(Y_train, np.sign(predictions_init_train)))\n",
    "print('accuracy test = ' , accuracy_score(Y_test, np.sign(predictions_init_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boucle d'apprentissage :\n",
    "\n",
    "* Définir le Nombre d'Itérations :\n",
    "\n",
    "Commencer par définir le nombre d'itérations ou arbres dans le modèle de boosting $n_estimators$. Utiliser les prédictions du modèle de base comme point de départ.\n",
    "\n",
    "* Boucle d'Apprentissage :\n",
    "\n",
    "- Prédiction : Utiliser le modèle actuel pour prédire les classes sur l'ensemble d'entraînement.\n",
    "- Calcul du Gradient : calculer le gradient de la fonction de perte par rapport aux prédictions, cela donne l'erreur résiduelle que le prochain modèle essaiera de prédire.\n",
    "- Ajustement du Nouveau Modèle : entraîner un nouveau modèle faible pour prédire ces gradients.\n",
    "- Mise à Jour des Prédictions : mettre à jour les prédictions globales en ajoutant les prédictions du nouveau modèle, pondérées par un taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8468426293658605\n",
      "accuracy train step 1 =  0.79125\n",
      "accuracy test step 1 =  0.66\n",
      "-0.847531705831603\n",
      "accuracy train step 2 =  0.79125\n",
      "accuracy test step 2 =  0.66\n",
      "-0.8482210753143655\n",
      "accuracy train step 3 =  0.79125\n",
      "accuracy test step 3 =  0.66\n",
      "-0.8489107375770909\n",
      "accuracy train step 4 =  0.79125\n",
      "accuracy test step 4 =  0.66\n",
      "-0.8496006923823737\n",
      "accuracy train step 5 =  0.79125\n",
      "accuracy test step 5 =  0.66\n",
      "-0.8502909394924609\n",
      "accuracy train step 6 =  0.79125\n",
      "accuracy test step 6 =  0.66\n",
      "-0.8509814786692528\n",
      "accuracy train step 7 =  0.79125\n",
      "accuracy test step 7 =  0.66\n",
      "-0.8516723096743033\n",
      "accuracy train step 8 =  0.79125\n",
      "accuracy test step 8 =  0.66\n",
      "-0.8523634322688214\n",
      "accuracy train step 9 =  0.79125\n",
      "accuracy test step 9 =  0.66\n",
      "-0.8530548462136712\n",
      "accuracy train step 10 =  0.79125\n",
      "accuracy test step 10 =  0.66\n",
      "-0.8537465512693735\n",
      "accuracy train step 11 =  0.79125\n",
      "accuracy test step 11 =  0.66\n",
      "-0.8544385471961056\n",
      "accuracy train step 12 =  0.79125\n",
      "accuracy test step 12 =  0.66\n",
      "-0.8551308337537027\n",
      "accuracy train step 13 =  0.79125\n",
      "accuracy test step 13 =  0.66\n",
      "-0.8558234107016587\n",
      "accuracy train step 14 =  0.79125\n",
      "accuracy test step 14 =  0.66\n",
      "-0.8565162777991265\n",
      "accuracy train step 15 =  0.79125\n",
      "accuracy test step 15 =  0.66\n",
      "-0.8572094348049194\n",
      "accuracy train step 16 =  0.79125\n",
      "accuracy test step 16 =  0.66\n",
      "-0.8579028814775111\n",
      "accuracy train step 17 =  0.79125\n",
      "accuracy test step 17 =  0.66\n",
      "-0.8585966175750372\n",
      "accuracy train step 18 =  0.79125\n",
      "accuracy test step 18 =  0.66\n",
      "-0.8592906428552956\n",
      "accuracy train step 19 =  0.79125\n",
      "accuracy test step 19 =  0.66\n",
      "-0.8599849570757471\n",
      "accuracy train step 20 =  0.79125\n",
      "accuracy test step 20 =  0.66\n",
      "-0.8606795599935168\n",
      "accuracy train step 21 =  0.79125\n",
      "accuracy test step 21 =  0.66\n",
      "-0.8613744513653941\n",
      "accuracy train step 22 =  0.79125\n",
      "accuracy test step 22 =  0.66\n",
      "-0.8620696309478344\n",
      "accuracy train step 23 =  0.79125\n",
      "accuracy test step 23 =  0.66\n",
      "-0.8627650984969587\n",
      "accuracy train step 24 =  0.79125\n",
      "accuracy test step 24 =  0.66\n",
      "-0.8634608537685554\n",
      "accuracy train step 25 =  0.79125\n",
      "accuracy test step 25 =  0.66\n",
      "-0.8641568965180808\n",
      "accuracy train step 26 =  0.79125\n",
      "accuracy test step 26 =  0.66\n",
      "-0.8648532265006595\n",
      "accuracy train step 27 =  0.79125\n",
      "accuracy test step 27 =  0.66\n",
      "-0.8655498434710858\n",
      "accuracy train step 28 =  0.79125\n",
      "accuracy test step 28 =  0.66\n",
      "-0.8662467471838238\n",
      "accuracy train step 29 =  0.79125\n",
      "accuracy test step 29 =  0.66\n",
      "-0.8669439373930089\n",
      "accuracy train step 30 =  0.79125\n",
      "accuracy test step 30 =  0.66\n",
      "-0.8676414138524481\n",
      "accuracy train step 31 =  0.79125\n",
      "accuracy test step 31 =  0.66\n",
      "-0.868339176315621\n",
      "accuracy train step 32 =  0.79125\n",
      "accuracy test step 32 =  0.66\n",
      "-0.8690372245356806\n",
      "accuracy train step 33 =  0.79125\n",
      "accuracy test step 33 =  0.66\n",
      "-0.8697355582654538\n",
      "accuracy train step 34 =  0.79125\n",
      "accuracy test step 34 =  0.66\n",
      "-0.8704341772574427\n",
      "accuracy train step 35 =  0.79125\n",
      "accuracy test step 35 =  0.66\n",
      "-0.8711330812638249\n",
      "accuracy train step 36 =  0.79125\n",
      "accuracy test step 36 =  0.66\n",
      "-0.8718322700364548\n",
      "accuracy train step 37 =  0.79125\n",
      "accuracy test step 37 =  0.66\n",
      "-0.8725317433268639\n",
      "accuracy train step 38 =  0.79125\n",
      "accuracy test step 38 =  0.66\n",
      "-0.8732315008862619\n",
      "accuracy train step 39 =  0.79125\n",
      "accuracy test step 39 =  0.66\n",
      "-0.8739315424655374\n",
      "accuracy train step 40 =  0.79125\n",
      "accuracy test step 40 =  0.66\n",
      "-0.8746318678152588\n",
      "accuracy train step 41 =  0.79125\n",
      "accuracy test step 41 =  0.66\n",
      "-0.8753324766856752\n",
      "accuracy train step 42 =  0.79125\n",
      "accuracy test step 42 =  0.66\n",
      "-0.8760333688267167\n",
      "accuracy train step 43 =  0.79125\n",
      "accuracy test step 43 =  0.66\n",
      "-0.8767345439879958\n",
      "accuracy train step 44 =  0.79125\n",
      "accuracy test step 44 =  0.66\n",
      "-0.877436001918808\n",
      "accuracy train step 45 =  0.79125\n",
      "accuracy test step 45 =  0.66\n",
      "-0.8781377423681325\n",
      "accuracy train step 46 =  0.79125\n",
      "accuracy test step 46 =  0.66\n",
      "-0.8788397650846331\n",
      "accuracy train step 47 =  0.79125\n",
      "accuracy test step 47 =  0.66\n",
      "-0.8795420698166593\n",
      "accuracy train step 48 =  0.79125\n",
      "accuracy test step 48 =  0.66\n",
      "-0.8802446563122462\n",
      "accuracy train step 49 =  0.79125\n",
      "accuracy test step 49 =  0.66\n",
      "-0.8809475243191167\n",
      "accuracy train step 50 =  0.79125\n",
      "accuracy test step 50 =  0.66\n",
      "-0.8816506735846811\n",
      "accuracy train step 51 =  0.79125\n",
      "accuracy test step 51 =  0.66\n",
      "-0.8823541038560384\n",
      "accuracy train step 52 =  0.79125\n",
      "accuracy test step 52 =  0.66\n",
      "-0.8830578148799774\n",
      "accuracy train step 53 =  0.79125\n",
      "accuracy test step 53 =  0.66\n",
      "-0.883761806402977\n",
      "accuracy train step 54 =  0.79125\n",
      "accuracy test step 54 =  0.66\n",
      "-0.8844660781712074\n",
      "accuracy train step 55 =  0.79125\n",
      "accuracy test step 55 =  0.66\n",
      "-0.8851706299305306\n",
      "accuracy train step 56 =  0.79125\n",
      "accuracy test step 56 =  0.66\n",
      "-0.8858754614265016\n",
      "accuracy train step 57 =  0.79125\n",
      "accuracy test step 57 =  0.66\n",
      "-0.886580572404369\n",
      "accuracy train step 58 =  0.79125\n",
      "accuracy test step 58 =  0.66\n",
      "-0.887285962609076\n",
      "accuracy train step 59 =  0.79125\n",
      "accuracy test step 59 =  0.66\n",
      "-0.8879916317852607\n",
      "accuracy train step 60 =  0.79125\n",
      "accuracy test step 60 =  0.66\n",
      "-0.8886975796772578\n",
      "accuracy train step 61 =  0.79125\n",
      "accuracy test step 61 =  0.66\n",
      "-0.8894038060290987\n",
      "accuracy train step 62 =  0.79125\n",
      "accuracy test step 62 =  0.66\n",
      "-0.8901103105845128\n",
      "accuracy train step 63 =  0.79125\n",
      "accuracy test step 63 =  0.66\n",
      "-0.890817093086928\n",
      "accuracy train step 64 =  0.79125\n",
      "accuracy test step 64 =  0.66\n",
      "-0.8915241532794717\n",
      "accuracy train step 65 =  0.79125\n",
      "accuracy test step 65 =  0.66\n",
      "-0.8922314909049716\n",
      "accuracy train step 66 =  0.79125\n",
      "accuracy test step 66 =  0.66\n",
      "-0.8929391057059566\n",
      "accuracy train step 67 =  0.79125\n",
      "accuracy test step 67 =  0.66\n",
      "-0.8936469974246576\n",
      "accuracy train step 68 =  0.79125\n",
      "accuracy test step 68 =  0.66\n",
      "-0.8943551658030084\n",
      "accuracy train step 69 =  0.79125\n",
      "accuracy test step 69 =  0.66\n",
      "-0.8950636105826465\n",
      "accuracy train step 70 =  0.79125\n",
      "accuracy test step 70 =  0.66\n",
      "-0.8957723315049138\n",
      "accuracy train step 71 =  0.79125\n",
      "accuracy test step 71 =  0.66\n",
      "-0.8964813283108576\n",
      "accuracy train step 72 =  0.79125\n",
      "accuracy test step 72 =  0.66\n",
      "-0.8971906007412316\n",
      "accuracy train step 73 =  0.79125\n",
      "accuracy test step 73 =  0.66\n",
      "-0.8979001485364964\n",
      "accuracy train step 74 =  0.79125\n",
      "accuracy test step 74 =  0.66\n",
      "-0.8986099714368205\n",
      "accuracy train step 75 =  0.79125\n",
      "accuracy test step 75 =  0.66\n",
      "-0.8993200691820815\n",
      "accuracy train step 76 =  0.79125\n",
      "accuracy test step 76 =  0.66\n",
      "-0.9000304415118662\n",
      "accuracy train step 77 =  0.79125\n",
      "accuracy test step 77 =  0.66\n",
      "-0.9007410881654722\n",
      "accuracy train step 78 =  0.79125\n",
      "accuracy test step 78 =  0.66\n",
      "-0.9014520088819082\n",
      "accuracy train step 79 =  0.79125\n",
      "accuracy test step 79 =  0.66\n",
      "-0.9021632033998954\n",
      "accuracy train step 80 =  0.79125\n",
      "accuracy test step 80 =  0.66\n",
      "-0.9028746714578677\n",
      "accuracy train step 81 =  0.79125\n",
      "accuracy test step 81 =  0.66\n",
      "-0.9035864127939732\n",
      "accuracy train step 82 =  0.79125\n",
      "accuracy test step 82 =  0.66\n",
      "-0.9042984271460747\n",
      "accuracy train step 83 =  0.79125\n",
      "accuracy test step 83 =  0.66\n",
      "-0.9050107142517504\n",
      "accuracy train step 84 =  0.79125\n",
      "accuracy test step 84 =  0.66\n",
      "-0.9057232738482953\n",
      "accuracy train step 85 =  0.79125\n",
      "accuracy test step 85 =  0.66\n",
      "-0.9064361056727217\n",
      "accuracy train step 86 =  0.79125\n",
      "accuracy test step 86 =  0.66\n",
      "-0.9071492094617599\n",
      "accuracy train step 87 =  0.79125\n",
      "accuracy test step 87 =  0.66\n",
      "-0.9078625849518596\n",
      "accuracy train step 88 =  0.79125\n",
      "accuracy test step 88 =  0.66\n",
      "-0.9085762318791901\n",
      "accuracy train step 89 =  0.79125\n",
      "accuracy test step 89 =  0.66\n",
      "-0.909290149979642\n",
      "accuracy train step 90 =  0.79125\n",
      "accuracy test step 90 =  0.66\n",
      "-0.910004338988827\n",
      "accuracy train step 91 =  0.79125\n",
      "accuracy test step 91 =  0.66\n",
      "-0.9107187986420799\n",
      "accuracy train step 92 =  0.79125\n",
      "accuracy test step 92 =  0.66\n",
      "-0.9114335286744586\n",
      "accuracy train step 93 =  0.79125\n",
      "accuracy test step 93 =  0.66\n",
      "-0.9121485288207454\n",
      "accuracy train step 94 =  0.79125\n",
      "accuracy test step 94 =  0.66\n",
      "-0.9128637988154477\n",
      "accuracy train step 95 =  0.79125\n",
      "accuracy test step 95 =  0.66\n",
      "-0.913579338392799\n",
      "accuracy train step 96 =  0.79125\n",
      "accuracy test step 96 =  0.66\n",
      "-0.9142951472867599\n",
      "accuracy train step 97 =  0.79125\n",
      "accuracy test step 97 =  0.66\n",
      "-0.9150112252310184\n",
      "accuracy train step 98 =  0.79125\n",
      "accuracy test step 98 =  0.66\n",
      "-0.9157275719589916\n",
      "accuracy train step 99 =  0.79125\n",
      "accuracy test step 99 =  0.66\n",
      "-0.9164441872038258\n",
      "accuracy train step 100 =  0.79125\n",
      "accuracy test step 100 =  0.66\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100 # Nombre d'itérations\n",
    "learning_rate = 2  # Taux d'apprentissage\n",
    "predictions_train = predictions_init_train\n",
    "predictions_test = predictions_init_test\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    \n",
    "    # a. Prédiction avec le modèle actuel\n",
    "    current_prediction_train = predictions_train\n",
    "\n",
    "    # b. Calcul du gradient de la fonction de perte\n",
    "    residuals = - grad_loss_reg_log(Y_train, current_prediction_train)  # gradient_of_loss_function doit être défini\n",
    "\n",
    "    # c. Entraînement d'un nouveau modèle sur les résidus\n",
    "    new_model = DecisionTreeRegressor(max_depth=4)\n",
    "    new_model.fit(X_train, residuals)\n",
    "\n",
    "    # d. Mise à jour des prédictions\n",
    "    predictions_train += learning_rate * new_model.predict(X_train)\n",
    "    print(predictions_train[0])\n",
    "    predictions_test += learning_rate * new_model.predict(X_test)\n",
    "    \n",
    "    print(f'accuracy train step {i+1} = ' , accuracy_score(Y_train, np.sign(predictions_train)))\n",
    "    print(f'accuracy test step {i+1} = ' , accuracy_score(Y_test, np.sign(predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison avec une implémentation du Gradient Boosting du type AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibotcazou/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy train: 1.0\n",
      "AdaBoost Accuracy test: 0.845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Création du modèle AdaBoost avec un arbre de décision comme modèle de base\n",
    "ada_model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=4),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Entraînement du modèle AdaBoost\n",
    "ada_model.fit(X_train, Y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de train\n",
    "ada_predictions_train = ada_model.predict(X_train)\n",
    "# Prédiction sur l'ensemble de test\n",
    "ada_predictions_test = ada_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Évaluation de la précision\n",
    "ada_accuracy_train = accuracy_score(Y_train, ada_predictions_train)\n",
    "print(f'AdaBoost Accuracy train: {ada_accuracy_train}')\n",
    "# Évaluation de la précision\n",
    "ada_accuracy_test = accuracy_score(Y_test, ada_predictions_test)\n",
    "print(f'AdaBoost Accuracy test: {ada_accuracy_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
